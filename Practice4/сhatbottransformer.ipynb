{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bP_9EKMuntJr",
        "outputId": "786596f1-6b0f-4e0e-984d-7769f5d47220"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"
          ]
        }
      ],
      "source": [
        "%pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "n3Pm2dvZoCU9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuRbIb-0oHu4",
        "outputId": "bb271683-f36c-46a1-cd7f-f4725a7dfbe7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "xmABgCijoSM_"
      },
      "outputs": [],
      "source": [
        "class Chatbot:\n",
        "    def __init__(self, model_name, tokenizer_name, data_file):\n",
        "        self.model_name = model_name\n",
        "        self.tokenizer_name = tokenizer_name\n",
        "        self.data_file = data_file\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "        self.dataset = None\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    def load_dataset(self):\n",
        "        self.dataset = pd.read_csv(self.data_file)\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)\n",
        "        text = text.lower()\n",
        "        text = ' '.join([word for word in text.split() if word not in self.stop_words])\n",
        "        text = ' '.join([self.lemmatizer.lemmatize(word) for word in text.split()])\n",
        "        return text\n",
        "\n",
        "    def load_model(self):\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(self.tokenizer_name)\n",
        "        self.model = GPT2LMHeadModel.from_pretrained(self.model_name)\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "    def train(self):\n",
        "        self.load_dataset()\n",
        "        self.load_model()\n",
        "\n",
        "        inputs = self.dataset['question'].tolist()\n",
        "        responses = self.dataset['answer'].tolist()\n",
        "\n",
        "        input_tokens = self.tokenizer.batch_encode_plus([self.preprocess_text(text) for text in inputs], padding=True, truncation=True, return_tensors='pt')\n",
        "        response_tokens = self.tokenizer.batch_encode_plus([self.preprocess_text(text) for text in responses], padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "        input_ids = input_tokens['input_ids']\n",
        "        attention_mask = input_tokens['attention_mask']\n",
        "        target_ids = response_tokens['input_ids']\n",
        "\n",
        "        self.model.train()\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model.to(device)\n",
        "\n",
        "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=1e-4)\n",
        "\n",
        "        num_epochs = 5\n",
        "        for epoch in range(num_epochs):\n",
        "            total_loss = 0\n",
        "            for i in range(len(input_ids)):\n",
        "                input_batch = input_ids[i].unsqueeze(0).to(device)\n",
        "                attention_mask_batch = attention_mask[i].unsqueeze(0).to(device)\n",
        "                target_batch = target_ids[i].unsqueeze(0).to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                outputs = self.model(input_ids=input_batch, attention_mask=attention_mask_batch, labels=target_batch)\n",
        "                loss = outputs.loss\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "            avg_loss = total_loss / len(input_ids)\n",
        "            print(f\"Epoch {epoch+1}: Average Loss = {avg_loss}\")\n",
        "\n",
        "    def respond(self, user_input):\n",
        "        try:\n",
        "            input_text = self.preprocess_text(user_input)\n",
        "            input_ids = self.tokenizer.encode(input_text, return_tensors='pt')\n",
        "            output_ids = self.model.generate(\n",
        "                    input_ids,\n",
        "                    max_length=300,\n",
        "                    num_beams=5,\n",
        "                    no_repeat_ngram_size=3,\n",
        "                    pad_token_id=self.tokenizer.eos_token_id,\n",
        "                    eos_token_id=self.tokenizer.eos_token_id,\n",
        "                    bos_token_id=self.tokenizer.bos_token_id,\n",
        "                )\n",
        "\n",
        "            response = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "            return response\n",
        "        except:\n",
        "            return \"I'm sorry, I don't have an answer for that.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPtKohGVo5Ee",
        "outputId": "5cb7cb1e-3028-4aad-cfaa-14975c68dc58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Average Loss = 2.392216528869115\n",
            "Epoch 2: Average Loss = 1.9270132882583062\n",
            "Epoch 3: Average Loss = 1.7246114112426771\n",
            "Epoch 4: Average Loss = 1.4437235058654074\n",
            "Epoch 5: Average Loss = 1.2070866096959818\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "    chatbot = Chatbot('microsoft/DialoGPT-medium', 'microsoft/DialoGPT-medium', 'chatbot.csv')\n",
        "    chatbot.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJMmRgv4pAav",
        "outputId": "30261373-174b-4604-cc42-8cbdc3548f27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello! I'm Бот. How can I help you?\n",
            "Пользователь: What is the meaning of life?\n",
            "Бот: I'm sorry, I don't have an answer for that.\n",
            "Пользователь: выход\n"
          ]
        }
      ],
      "source": [
        "print(\"Hello! I'm Бот. How can I help you?\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"Пользователь: \")\n",
        "\n",
        "    if user_input.lower() == 'выход':\n",
        "        break\n",
        "    else:\n",
        "        response = chatbot.respond(user_input)\n",
        "        print(f\"Бот: {response}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
